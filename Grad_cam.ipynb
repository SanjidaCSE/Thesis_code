{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2645886,"sourceType":"datasetVersion","datasetId":1608934},{"sourceId":6496514,"sourceType":"datasetVersion","datasetId":3754858},{"sourceId":440361,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":358583,"modelId":379899},{"sourceId":440592,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":358608,"modelId":379923}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install imutils\n\nimport tensorflow as tf\nfrom sklearn.utils.class_weight import compute_class_weight\nimport matplotlib.pyplot as plt\nimport imutils\nimport pathlib\nimport time\nimport PIL as pil\nimport shutil\nfrom tqdm import tqdm\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport gc\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport itertools\nimport joblib\n\n\n\nIMAGE_SIZE = (224, 224)\nBASE_LR = 2e-5\nEPOCH = 20\nBATCH_SIZE = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:32:59.260677Z","iopub.execute_input":"2025-06-19T00:32:59.260890Z","iopub.status.idle":"2025-06-19T00:33:21.298271Z","shell.execute_reply.started":"2025-06-19T00:32:59.260853Z","shell.execute_reply":"2025-06-19T00:33:21.297267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def crop_img(img, image_size=(224, 224)):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular region\n    Resizes to the target image size using bicubic interpolation\n    \"\"\"\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n\n    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n    thresh = cv2.erode(thresh, None, iterations=2)\n    thresh = cv2.dilate(thresh, None, iterations=2)\n\n    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    \n    if len(cnts) == 0:\n        print(\"Warning: No contours found, returning resized original image.\")\n        return cv2.resize(img, image_size, interpolation=cv2.INTER_CUBIC)\n    \n    c = max(cnts, key=cv2.contourArea)\n\n    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n    extRight = tuple(c[c[:, :, 0].argmax()][0])\n    extTop = tuple(c[c[:, :, 1].argmin()][0])\n    extBot = tuple(c[c[:, :, 1].argmax()][0])\n\n    ADD_PIXELS = 0  # Can be tuned for padding\n    new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n    \n    # Resize using bicubic interpolation\n    new_img = cv2.resize(new_img, image_size, interpolation=cv2.INTER_CUBIC)\n    \n    return new_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:33:27.438848Z","iopub.execute_input":"2025-06-19T00:33:27.439504Z","iopub.status.idle":"2025-06-19T00:33:27.448455Z","shell.execute_reply.started":"2025-06-19T00:33:27.439471Z","shell.execute_reply":"2025-06-19T00:33:27.447599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def image_preprocessing(source_dir, saved_root_dir, image_size=(224, 224), channels=3):\n    if not os.path.exists(source_dir):\n        raise Exception(f\"Source directory: {source_dir} does not exist\")\n    if not os.path.isdir(source_dir):\n        raise Exception(f\"Source path: {source_dir} is not a directory\")\n\n    if not os.path.exists(saved_root_dir):\n        os.makedirs(saved_root_dir)\n        \n    source_dir_path = pathlib.Path(source_dir)\n    \n    for p in tqdm(source_dir_path.iterdir(), desc=\"Processing folders\"):\n        dir_name = str(p).split(\"/\")[-1]\n        for fp in p.iterdir():\n            filename = str(fp).split(\"/\")[-1]\n\n            img = tf.io.read_file(str(fp))\n            img = tf.image.decode_jpeg(img, channels=channels)\n            img = crop_img(img.numpy(), image_size)\n            img = pil.Image.fromarray(img)\n\n            saved_dist_dir = os.path.join(saved_root_dir, dir_name)\n            if not os.path.exists(saved_dist_dir):\n                os.makedirs(saved_dist_dir)\n\n            img_dist_path = os.path.join(saved_dist_dir, filename)\n            img.save(img_dist_path)\n    print(f\"\\nâœ… All images processed and saved to: {saved_root_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:34:06.742073Z","iopub.execute_input":"2025-06-19T00:34:06.742540Z","iopub.status.idle":"2025-06-19T00:34:06.749646Z","shell.execute_reply.started":"2025-06-19T00:34:06.742508Z","shell.execute_reply":"2025-06-19T00:34:06.748707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_preprocessing(\"/kaggle/input/brain-tumor-mri-dataset/Training\",\n                   \"/kaggle/working/brain-tumor-mri-dataset/Training\",\n                   image_size=IMAGE_SIZE)\n\nimage_preprocessing(\"/kaggle/input/brain-tumor-mri-dataset/Testing\",\n                   \"/kaggle/working/brain-tumor-mri-dataset/Testing\",\n                   image_size=IMAGE_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:34:11.903853Z","iopub.execute_input":"2025-06-19T00:34:11.904515Z","iopub.status.idle":"2025-06-19T00:35:18.212941Z","shell.execute_reply.started":"2025-06-19T00:34:11.904484Z","shell.execute_reply":"2025-06-19T00:35:18.212052Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Datasets","metadata":{"id":"YuJXo3b-f7PX"}},{"cell_type":"code","source":"root_dir_path = \"/kaggle/working/brain-tumor-mri-dataset\"","metadata":{"id":"nLLkxrD0ivtJ","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:35:22.631108Z","iopub.execute_input":"2025-06-19T00:35:22.631801Z","iopub.status.idle":"2025-06-19T00:35:22.635671Z","shell.execute_reply.started":"2025-06-19T00:35:22.631771Z","shell.execute_reply":"2025-06-19T00:35:22.634745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = tf.keras.utils.image_dataset_from_directory(\n    \"/kaggle/working/brain-tumor-mri-dataset/Training\",\n    label_mode=\"categorical\",\n    batch_size=32,\n    image_size=IMAGE_SIZE,\n    seed=42\n)\n\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    \"/kaggle/input/brain-tumor-mri-dataset/Testing\",  # original, not preprocessed\n    label_mode=\"categorical\",\n    image_size=IMAGE_SIZE,\n    batch_size=32,\n    shuffle=False  # Important for consistent evaluation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:35:27.910662Z","iopub.execute_input":"2025-06-19T00:35:27.911025Z","iopub.status.idle":"2025-06-19T00:35:28.561956Z","shell.execute_reply.started":"2025-06-19T00:35:27.910996Z","shell.execute_reply":"2025-06-19T00:35:28.561332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_ds.class_names)\nprint(test_ds.class_names)\n\ncls_to_id = {c:i for i, c in enumerate(train_ds.class_names)}\nprint(cls_to_id)\nid_to_cls = {i:c for i, c in enumerate(train_ds.class_names)}\nprint(id_to_cls)","metadata":{"id":"8Q3yArLCf7Pe","outputId":"6a76410d-f086-4531-b6cc-6599f0fb2512","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:35:33.190102Z","iopub.execute_input":"2025-06-19T00:35:33.190901Z","iopub.status.idle":"2025-06-19T00:35:33.196194Z","shell.execute_reply.started":"2025-06-19T00:35:33.190861Z","shell.execute_reply":"2025-06-19T00:35:33.195336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"class_to_id.txt\", \"w\") as f:\n    for k, v in cls_to_id.items():\n        f.write(f\"{k}\\t{v}\\n\")\n\nwith open(\"id_to_class.txt\", \"w\") as f:\n    for k, v in id_to_cls.items():\n        f.write(f\"{k}\\t{v}\\n\")","metadata":{"id":"uPjSeagdf7Pf","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:35:36.576332Z","iopub.execute_input":"2025-06-19T00:35:36.577108Z","iopub.status.idle":"2025-06-19T00:35:36.582510Z","shell.execute_reply.started":"2025-06-19T00:35:36.577079Z","shell.execute_reply":"2025-06-19T00:35:36.581591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"class_to_id.txt\", \"r\") as f:\n    for line in f.readlines():\n        cls, label = line.replace(\"\\n\",\"\").split(\"\\t\")\n        print(cls, int(label))\nprint(\"\\n\")\nwith open(\"id_to_class.txt\", \"r\") as f:\n    for line in f.readlines():\n        label, cls = line.replace(\"\\n\",\"\").split(\"\\t\")\n        print(int(label), cls)","metadata":{"id":"XLvvdUm6f7Pf","outputId":"b2aa2166-813d-42a6-fb31-2d166c69afbf","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:35:39.930995Z","iopub.execute_input":"2025-06-19T00:35:39.931374Z","iopub.status.idle":"2025-06-19T00:35:39.937830Z","shell.execute_reply.started":"2025-06-19T00:35:39.931345Z","shell.execute_reply":"2025-06-19T00:35:39.937008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def class_weight_from_one_hot(ds):\n    class_labels = []\n    if ds.__class__.__name__ == \"_BatchDataset\":\n        ds = ds.unbatch()\n    \n    for _, onehot in ds:\n        class_labels.append(tf.argmax(onehot).numpy())\n    \n    unique_classes = np.unique(class_labels)\n    class_weights = compute_class_weight(class_weight=\"balanced\", \n                                         classes=unique_classes,\n                                         y=class_labels)\n    return {i:w for i, w in enumerate(class_weights)}\n\n    \nclass_weights = class_weight_from_one_hot(train_ds)\nprint(class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:35:43.978329Z","iopub.execute_input":"2025-06-19T00:35:43.979161Z","iopub.status.idle":"2025-06-19T00:35:48.357857Z","shell.execute_reply.started":"2025-06-19T00:35:43.979129Z","shell.execute_reply":"2025-06-19T00:35:48.356946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for images, labels in train_ds.take(1):\n    image, label = images[0], labels[0]\n    plt.figure()\n    plt.imshow(tf.cast(image, tf.uint8))\n    plt.title(train_ds.class_names[tf.argmax(label).numpy()])\n    plt.show()","metadata":{"id":"f2EnMhWWf7Pf","outputId":"b33194ba-2d4f-4efa-ce72-84677d5d64ba","trusted":true,"execution":{"iopub.status.busy":"2025-06-18T19:30:01.076178Z","iopub.execute_input":"2025-06-18T19:30:01.076498Z","iopub.status.idle":"2025-06-18T19:30:01.545847Z","shell.execute_reply.started":"2025-06-18T19:30:01.076472Z","shell.execute_reply":"2025-06-18T19:30:01.544994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GAM(tf.keras.layers.Layer):\n    def __init__(self, reduction_ratio=16, **kwargs):\n        super(GAM, self).__init__(**kwargs)\n        self.reduction_ratio = reduction_ratio\n\n    def build(self, input_shape):\n        channels = input_shape[-1]\n        self.channel_mlp = tf.keras.Sequential([\n            tf.keras.layers.Dense(channels // self.reduction_ratio, activation='relu'),\n            tf.keras.layers.Dense(channels)\n        ])\n        self.spatial_conv = tf.keras.Sequential([\n            tf.keras.layers.Conv2D(channels // self.reduction_ratio, 7, padding='same', activation='relu'),\n            tf.keras.layers.Conv2D(1, 7, padding='same', activation='sigmoid')\n        ])\n\n    def call(self, inputs):\n        # Channel attention\n        channel_att = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n        channel_att = self.channel_mlp(channel_att)\n\n        # Spatial attention\n        spatial_att = self.spatial_conv(inputs)\n\n        return inputs * channel_att * spatial_att\n\nclass ECA(tf.keras.layers.Layer):\n    def __init__(self, gamma=2, b=1, **kwargs):\n        super(ECA, self).__init__(**kwargs)\n        self.gamma = gamma\n        self.b = b\n\n    def build(self, input_shape):\n        channels = input_shape[-1]\n        self.kernel_size = int(abs((tf.math.log(tf.cast(channels, tf.float32), 2) + self.b) / self.gamma))\n        self.kernel_size = self.kernel_size if self.kernel_size % 2 else self.kernel_size + 1\n        self.conv = tf.keras.layers.Conv1D(1, kernel_size=self.kernel_size, padding='same', use_bias=False)\n\n    def call(self, inputs):\n        # Global Average Pooling (Keep Channel Dimension)\n        x = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n\n        # Reshape for Conv1D\n        x = tf.squeeze(x, axis=[1, 2])\n        x = tf.expand_dims(x, axis=-1)\n\n        # Apply 1D Convolution for Channel Attention\n        x = self.conv(x)\n        x = tf.sigmoid(x)\n\n        # Reshape to match original input dimensions\n        x = tf.reshape(x, [-1, 1, 1, inputs.shape[-1]])\n\n        return inputs * x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:35:58.865667Z","iopub.execute_input":"2025-06-19T00:35:58.866521Z","iopub.status.idle":"2025-06-19T00:35:58.876037Z","shell.execute_reply.started":"2025-06-19T00:35:58.866486Z","shell.execute_reply":"2025-06-19T00:35:58.875194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model \n\nEfficientNetV2","metadata":{"id":"bFFWroJEf7Pg"}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Define or import GAM and ECA before loading\n# from your_custom_layers import GAM, ECA\n\nmodel = load_model(\n    '/kaggle/input/keras_m/tensorflow2/default/1/brain_tumor_detector.keras',\n    custom_objects={'GAM': GAM, 'ECA': ECA}\n)\n\nfor layer in model.layers:\n    print(layer.name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T01:59:32.156739Z","iopub.execute_input":"2025-06-19T01:59:32.157109Z","iopub.status.idle":"2025-06-19T01:59:37.276835Z","shell.execute_reply.started":"2025-06-19T01:59:32.157081Z","shell.execute_reply":"2025-06-19T01:59:37.275862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport random\nfrom IPython.display import Image\n# import imutils   \n\n\nimport keras\nimport tensorflow.keras as K\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator, array_to_img, img_to_array\nfrom tensorflow.keras.applications import EfficientNetB1\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Flatten, Dense, Conv2D, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T01:43:09.780186Z","iopub.execute_input":"2025-06-19T01:43:09.780560Z","iopub.status.idle":"2025-06-19T01:43:09.786551Z","shell.execute_reply.started":"2025-06-19T01:43:09.780531Z","shell.execute_reply":"2025-06-19T01:43:09.785641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"has_conv_layers = any(isinstance(layer, K.layers.Conv2D) for layer in model.layers)\nprint(f\"Model has convolutional layers: {has_conv_layers}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T01:43:15.755403Z","iopub.execute_input":"2025-06-19T01:43:15.755741Z","iopub.status.idle":"2025-06-19T01:43:15.761110Z","shell.execute_reply.started":"2025-06-19T01:43:15.755715Z","shell.execute_reply":"2025-06-19T01:43:15.760145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def VizGradCAM(model, image, interpolant=0.5, plot_results=True):\n    \"\"\"Fixed Grad-CAM implementation for your model\"\"\"\n    assert (interpolant > 0 and interpolant < 1), \"Heatmap Interpolation Must Be Between 0 - 1\"\n    \n    try:\n        # STEP 1: Preprocess image and make prediction\n        original_img = np.asarray(image, dtype=np.float32)\n        img = np.expand_dims(original_img, axis=0)\n        prediction = model.predict(img, verbose=0)\n        prediction_idx = np.argmax(prediction)\n\n        # STEP 2: Get the ECA layer output (last conv-like layer)\n        eca_layer = model.get_layer('eca_5')\n        \n        # Create gradient model\n        grad_model = Model(\n            inputs=model.inputs,\n            outputs=[eca_layer.output, model.output]\n        )\n        \n        # STEP 3: Compute gradients\n        with tf.GradientTape() as tape:\n            conv_output, preds = grad_model(img, training=False)\n            loss = preds[:, prediction_idx]\n\n        # Compute gradients\n        grads = tape.gradient(loss, conv_output)[0]  # Take first (and only) batch element\n        pooled_grads = tf.reduce_mean(grads, axis=(0, 1))  # Average over spatial dimensions\n        \n        # Weight the feature maps by gradient importance\n        conv_output = conv_output[0].numpy()  # Shape: (8, 8, 1280)\n        pooled_grads = pooled_grads.numpy()   # Shape: (1280,)\n        \n        # Multiply each feature map by its corresponding gradient mean\n        for i in range(pooled_grads.shape[0]):\n            conv_output[:, :, i] *= pooled_grads[i]\n        \n        # Create heatmap by averaging across channels\n        heatmap = np.mean(conv_output, axis=-1)\n        \n        # Post-process heatmap\n        heatmap = np.maximum(heatmap, 0)\n        heatmap /= np.max(heatmap)  # Normalize between 0-1\n        heatmap = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n        heatmap = np.uint8(255 * heatmap)\n\n        # Apply colormap\n        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n        \n        # Prepare original image\n        original_img_uint8 = np.uint8(255 * (original_img - original_img.min()) / \n                            (original_img.max() - original_img.min()))\n        \n        # Superimpose\n        superimposed_img = cv2.addWeighted(original_img_uint8, interpolant, \n                                         heatmap, 1 - interpolant, 0)\n        \n        # Display\n        plt.figure(figsize=(10, 5))\n        \n        plt.subplot(1, 2, 1)\n        plt.imshow(original_img_uint8)\n        plt.title(\"Original Image\")\n        plt.axis('off')\n        \n        plt.subplot(1, 2, 2)\n        plt.imshow(superimposed_img)\n        plt.title(\"Grad-CAM\")\n        plt.axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n\n    except Exception as e:\n        print(f\"Error in VizGradCAM: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        if not plot_results:\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T01:59:49.551415Z","iopub.execute_input":"2025-06-19T01:59:49.551731Z","iopub.status.idle":"2025-06-19T01:59:49.563012Z","shell.execute_reply.started":"2025-06-19T01:59:49.551708Z","shell.execute_reply":"2025-06-19T01:59:49.562098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your image (without augmentation)\ndef load_image(path):\n    img = tf.keras.preprocessing.image.load_img(path, target_size=IMAGE_SIZE)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    return img\n\n# Apply Grad-CAM\nmri_image = load_image(\"/kaggle/input/brain-tumor-mri-dataset/Testing/glioma/Te-glTr_0000.jpg\")\nVizGradCAM(model, mri_image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T01:59:55.889053Z","iopub.execute_input":"2025-06-19T01:59:55.889699Z","iopub.status.idle":"2025-06-19T01:59:58.228543Z","shell.execute_reply.started":"2025-06-19T01:59:55.889665Z","shell.execute_reply":"2025-06-19T01:59:58.227708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply Grad-CAM\nmri_image = load_image(\"/kaggle/input/brain-tumor-mri-dataset/Testing/meningioma/Te-meTr_0001.jpg\")\nVizGradCAM(model, mri_image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T01:45:14.015100Z","iopub.execute_input":"2025-06-19T01:45:14.015463Z","iopub.status.idle":"2025-06-19T01:45:14.769920Z","shell.execute_reply.started":"2025-06-19T01:45:14.015434Z","shell.execute_reply":"2025-06-19T01:45:14.769137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply Grad-CAM\nmri_image = load_image(\"/kaggle/input/brain-tumor-mri-dataset/Testing/notumor/Te-noTr_0006.jpg\")\nVizGradCAM(model, mri_image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T01:47:42.198619Z","iopub.execute_input":"2025-06-19T01:47:42.199590Z","iopub.status.idle":"2025-06-19T01:47:42.939107Z","shell.execute_reply.started":"2025-06-19T01:47:42.199546Z","shell.execute_reply":"2025-06-19T01:47:42.938062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply Grad-CAM\nmri_image = load_image(\"/kaggle/input/brain-tumor-mri-dataset/Testing/pituitary/Te-piTr_0001.jpg\")\nVizGradCAM(model, mri_image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T01:48:32.852131Z","iopub.execute_input":"2025-06-19T01:48:32.853141Z","iopub.status.idle":"2025-06-19T01:48:33.757074Z","shell.execute_reply.started":"2025-06-19T01:48:32.853107Z","shell.execute_reply":"2025-06-19T01:48:33.756089Z"}},"outputs":[],"execution_count":null}]}